{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle, validation\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-09T14:02:29.680589Z","iopub.execute_input":"2022-05-09T14:02:29.681491Z","iopub.status.idle":"2022-05-09T14:02:32.747770Z","shell.execute_reply.started":"2022-05-09T14:02:29.681456Z","shell.execute_reply":"2022-05-09T14:02:32.746548Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Use GPU if it's available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:32.750274Z","iopub.execute_input":"2022-05-09T14:02:32.750666Z","iopub.status.idle":"2022-05-09T14:02:32.843101Z","shell.execute_reply.started":"2022-05-09T14:02:32.750622Z","shell.execute_reply":"2022-05-09T14:02:32.840721Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read in training data and training labels\ntrue_X = np.array( pd.read_csv('../input/food-iml/train_triplets.txt', sep=' ',header=None,dtype=str) )\ntrue_Y = np.ones(true_X.shape[0])\n\ntest_X = np.array( pd.read_csv('../input/food-iml/test_triplets.txt',sep=' ',header=None,dtype=str) )\ntest_y = np.ones(test_X.shape[0])\n\n# Invert the position of B and C\nfalse_X = true_X[:, [0, 2, 1]] \nfalse_Y = np.zeros(true_X.shape[0])\n\nX = np.concatenate((true_X, false_X), axis=0)\ny = np.concatenate((true_Y, false_Y), axis=0)\n\nX_train = X\ny_train = y\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:34.625358Z","iopub.execute_input":"2022-05-09T14:02:34.625684Z","iopub.status.idle":"2022-05-09T14:02:34.789586Z","shell.execute_reply.started":"2022-05-09T14:02:34.625652Z","shell.execute_reply":"2022-05-09T14:02:34.788552Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# scale images to 224 x 224 and normalize\ndata_transform = transforms.Compose([\n    transforms.Resize([224,224]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ndata_folder = Path('../input/food-iml/food/food')\nimg_dict = {}\n# image_files = list(data_folder.iterdir())\n# Image.open(image_files[0])\nfor img_name in tqdm(os.listdir(data_folder)):\n#     img = Image.open(os.path.join(data_folder, img_name))\n    img_dict[img_name.split('.')[0]] = data_folder/img_name\n#     img.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:35.630617Z","iopub.execute_input":"2022-05-09T14:02:35.631484Z","iopub.status.idle":"2022-05-09T14:02:36.147510Z","shell.execute_reply.started":"2022-05-09T14:02:35.631440Z","shell.execute_reply":"2022-05-09T14:02:36.146418Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 10000/10000 [00:00<00:00, 47568.87it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# setup dataset\n\nclass DataSet():\n    def __init__(self, X_pics, labels, img_dict, img_transform):\n        self.X_pics = X_pics # string i.e. '01893', 03404, 04972 stand for A B C\n        self.labels = labels # number, 0 or 1\n        self.img_dict = img_dict # folder containing images\\\n        self.img_transform = img_transform\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        \n        # read in image A, B, C\n        # should return (Tensor[image_channels, image_height, image_width])\n        img_A = self.img_transform(Image.open(img_dict[self.X_pics[idx, 0]]))\n        img_B = self.img_transform(Image.open(img_dict[self.X_pics[idx, 1]]))\n        img_C = self.img_transform(Image.open(img_dict[self.X_pics[idx, 2]]))\n        label = self.labels[idx]\n\n\n        return (img_A, img_B, img_C), label\n\n\n#setup loader\nbatch_size = 8\ntrainset = DataSet(X_train, y_train, img_dict, data_transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=False)\n\nvalidset = DataSet(X_valid, y_valid, img_dict, data_transform)\nvalidationloader = DataLoader(validset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:37.576633Z","iopub.execute_input":"2022-05-09T14:02:37.577217Z","iopub.status.idle":"2022-05-09T14:02:37.588727Z","shell.execute_reply.started":"2022-05-09T14:02:37.577180Z","shell.execute_reply":"2022-05-09T14:02:37.587124Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__() # initialize the torch.nn.Module first\n        \n        self.net = models.densenet121(pretrained=True)\n        num_feature_channels = self.net.classifier.in_features\n        print(num_feature_channels)\n        self.relu = nn.ReLU(inplace=True)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(num_feature_channels*2 , 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n\n    def forward(self, pic_A, pic_B, pic_C):\n        # B x 1024 x 7 x 7 - > B x 1024 x 1\n        mod_A = nn.functional.adaptive_avg_pool2d(self.relu(self.net.features(pic_A)), (1,1)) \n        mod_A = torch.flatten(mod_A, 1)\n        mod_B = nn.functional.adaptive_avg_pool2d(self.relu(self.net.features(pic_B)), (1,1)) \n        mod_B = torch.flatten(mod_B, 1)\n        mod_C = nn.functional.adaptive_avg_pool2d(self.relu(self.net.features(pic_C)), (1,1))\n        mod_C = torch.flatten(mod_C, 1)\n        diff_vec = torch.cat((mod_A - mod_B, mod_A - mod_C), 1) # BS x 2048 x 1\n        out = self.classifier(diff_vec)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:39.230620Z","iopub.execute_input":"2022-05-09T14:02:39.231416Z","iopub.status.idle":"2022-05-09T14:02:39.243576Z","shell.execute_reply.started":"2022-05-09T14:02:39.231340Z","shell.execute_reply":"2022-05-09T14:02:39.242393Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ConvNet2(nn.Module):\n    def __init__(self):\n        super().__init__() # initialize the torch.nn.Module first\n        \n        self.resnet = models.resnet18(pretrained=True)\n        #num_feature_channels = self.net.classifier.in_features\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 30)\n        #print(num_feature_channels)\n        #self.relu = nn.ReLU(inplace=True)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(60, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n\n    def forward(self, pic_A, pic_B, pic_C):\n        mod_A = self.resnet(pic_A)\n        mod_B = self.resnet(pic_B)\n        mod_C = self.resnet(pic_C)\n        diff_vec = torch.cat((mod_A - mod_B, mod_A - mod_C), 1) # BS x 60\n        out = self.classifier(diff_vec)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:40.490561Z","iopub.execute_input":"2022-05-09T14:02:40.491277Z","iopub.status.idle":"2022-05-09T14:02:40.499896Z","shell.execute_reply.started":"2022-05-09T14:02:40.491242Z","shell.execute_reply":"2022-05-09T14:02:40.498862Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Only train the classifier parameters, feature parameters are frozen\nmodel = ConvNet2().to(device)\nmodel.train()\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\nfor ii, ((a,b,c), labels) in enumerate(trainloader):\n\n    # Move input and label tensors to the GPU\n    a, b, c, labels = a.to(device), b.to(device), c.to(device), labels.to(device)\n\n    start = time.time()\n\n    outputs = model.forward(a,b,c).flatten().float()\n    print(outputs)\n    print(labels)\n    loss = criterion(outputs, labels.float())\n    loss.backward()\n    optimizer.step()\n\n    if ii==3:\n        break\n\nprint(f\"Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:02:41.774993Z","iopub.execute_input":"2022-05-09T14:02:41.775314Z","iopub.status.idle":"2022-05-09T14:02:54.951333Z","shell.execute_reply.started":"2022-05-09T14:02:41.775268Z","shell.execute_reply":"2022-05-09T14:02:54.950038Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/44.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89dc3b15fc4041d592d9ce628b8c0843"}},"metadata":{}},{"name":"stdout","text":"tensor([0.4732, 0.4680, 0.4791, 0.4719, 0.4964, 0.4616, 0.4424, 0.4467],\n       device='cuda:0', grad_fn=<ViewBackward>)\ntensor([0., 0., 1., 1., 0., 1., 1., 1.], device='cuda:0', dtype=torch.float64)\ntensor([0.4761, 0.4915, 0.4387, 0.4568, 0.4219, 0.4779, 0.5008, 0.4773],\n       device='cuda:0', grad_fn=<ViewBackward>)\ntensor([1., 0., 0., 0., 1., 1., 0., 1.], device='cuda:0', dtype=torch.float64)\ntensor([0.4948, 0.4624, 0.4695, 0.4863, 0.5120, 0.4400, 0.4748, 0.4941],\n       device='cuda:0', grad_fn=<ViewBackward>)\ntensor([1., 1., 1., 0., 1., 1., 0., 0.], device='cuda:0', dtype=torch.float64)\ntensor([0.5043, 0.4994, 0.4929, 0.4751, 0.4896, 0.5015, 0.4177, 0.4315],\n       device='cuda:0', grad_fn=<ViewBackward>)\ntensor([0., 0., 1., 1., 1., 0., 0., 1.], device='cuda:0', dtype=torch.float64)\nDevice = cuda; Time per batch: 0.027 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(n_epochs,trainloader,validationloader, model, optim, criterion, save_path):\n    \"\"\"returns trained model\"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf \n    running_loss=0\n    \n  \n    for epoch in range(n_epochs):\n        \n        \n        for (pic_A, pic_B, pic_C), label in trainloader:\n        \n            \n        # Move input and label tensors to the default device\n            # get convolved pic A,B,C\n            optim.zero_grad()\n            pic_A = pic_A.to(device)\n            pic_B = pic_B.to(device)\n            pic_C = pic_C.to(device)\n            target = label.unsqueeze(1).to(device).float()\n            out = model(pic_A, pic_B, pic_C).float()\n            loss = criterion(out, target)\n\n            loss.backward()\n            optim.step()\n        \n            running_loss += loss.item()\n            print(loss.item())\n        \n        \n        model.eval()\n        valid_loss=0\n        accuracy=0\n        with torch.no_grad():\n            for (pic_A, pic_B, pic_C), labels in validationloader:\n                pic_A = pic_A.to(device)\n                pic_B = pic_B.to(device)\n                pic_C = pic_C.to(device)\n                target = label.unsqueeze(1).to(device)\n                logps = model(inputs)\n                batch_loss = criterion(logps, labels)\n                valid_loss += batch_loss.item()\n                    \n                # Calculate accuracy\n                \n                top_p, top_class = logps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n           \n        \n            if valid_loss <= valid_loss_min:\n                print(\"Validation loss decreased  Saving model\")\n                torch.save(model.state_dict(),'food_classifier_densenet121_noise.pt')\n                valid_loss_min=valid_loss\n                \n                    \n            \n            \n            print(f\"Device = cuda; Time per batch: {(time.time() - start):.3f} seconds\")       \n            print(f\"Epoch /{n_epochs}.. \"\n                  f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n                  f\"Test loss: {valid_loss/len(validationloader):.3f}.. \"\n                  f\"Test accuracy: {accuracy/len(validationloader):.3f}\")\n            running_loss = 0\n            model.train()    ","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:04:58.963409Z","iopub.execute_input":"2022-05-09T14:04:58.963713Z","iopub.status.idle":"2022-05-09T14:04:58.978834Z","shell.execute_reply.started":"2022-05-09T14:04:58.963680Z","shell.execute_reply":"2022-05-09T14:04:58.977797Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = ConvNet2().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\ncriterion = nn.BCELoss()\n\ntrain(8,trainloader,validationloader, model, optimizer, criterion,'model_vowel_consonant.pt')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:05:00.679890Z","iopub.execute_input":"2022-05-09T14:05:00.680329Z","iopub.status.idle":"2022-05-09T14:05:03.767209Z","shell.execute_reply.started":"2022-05-09T14:05:00.680293Z","shell.execute_reply":"2022-05-09T14:05:03.765341Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0.7090262174606323\n0.7161120176315308\n0.7130205631256104\n0.6807634830474854\n0.7396259307861328\n0.683922529220581\n0.6695401668548584\n0.6818229556083679\n0.6726043224334717\n0.6523041725158691\n0.6750168800354004\n0.6766279339790344\n0.7219895124435425\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/1956746853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'model_vowel_consonant.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_34/1222305722.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, trainloader, validationloader, model, optim, criterion, save_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpic_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_34/1963432468.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# read in image A, B, C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# should return (Tensor[image_channels, image_height, image_width])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimg_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mimg_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimg_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}